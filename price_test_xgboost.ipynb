{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pandas_ta as ta\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_filepath = (\"place here directory for your destination database\")\n",
    "engine = create_engine(f\"sqlite:///{db_filepath}\")\n",
    "\n",
    "#database for saving parametrization of tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ticker_list = ['insert tickers you want to check]\n",
    "\n",
    "#ticker list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'min_child_weight': [i for i in range(0, 6)],\n",
    "    'reg_alpha': [i for i in range(0, 16)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(ticker, startdate):\n",
    "    # preparing dataset for training - data is downloaded with yfinance lib, information about tickers comes from Yahoo Finance\n",
    "    df = yf.download(ticker, startdate)\n",
    "    df.drop(columns=(['Adj Close', 'Volume']), inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    #creation of column features\n",
    "    data['previous_close'] = data['Close'].shift(1)\n",
    "    data['previous_open'] = data['Open'].shift(1)\n",
    "    data['previous_high'] = data['High'].shift(1)\n",
    "    data['previous_low'] = data['Low'].shift(1)\n",
    "    data['close_lag_1'] = data['previous_close'].shift(1) / data['previous_close'] - 1\n",
    "    data['close_lag_7'] = data['previous_close'].shift(7) / data['previous_close'] - 1\n",
    "    data['close_lag_10'] = data['previous_close'].shift(10) / data['previous_close'] - 1\n",
    "    data['close_lag_14'] = data['previous_close'].shift(14) / data['previous_close'] - 1\n",
    "    data['close_lag_21'] = data['previous_close'].shift(21) / data['previous_close'] - 1\n",
    "    data['close_lag_31'] = data['previous_close'].shift(31) / data['previous_close'] - 1\n",
    "    data['close_lag_50'] = data['previous_close'].shift(50) / data['previous_close'] - 1\n",
    "    data['close_lag_100'] = data['previous_close'].shift(100) / data['previous_close'] - 1\n",
    "    data['open_lag_1'] = data['previous_open'].shift(1) / data['previous_open'] - 1\n",
    "    data['open_lag_7'] = data['previous_open'].shift(7) / data['previous_open'] - 1\n",
    "    data['open_lag_10'] = data['previous_open'].shift(10) / data['previous_open'] - 1\n",
    "    data['open_lag_14'] = data['previous_open'].shift(14) / data['previous_open'] - 1\n",
    "    data['open_lag_21'] = data['previous_open'].shift(21) / data['previous_open'] - 1\n",
    "    data['open_lag_31'] = data['previous_open'].shift(31) / data['previous_open'] - 1\n",
    "    data['open_lag_50'] = data['previous_open'].shift(50) / data['previous_open'] - 1\n",
    "    data['open_lag_100'] = data['previous_open'].shift(100) / data['previous_open'] - 1\n",
    "    data['high_lag_1'] = data['previous_high'].shift(1) / data['previous_high'] - 1\n",
    "    data['high_lag_7'] = data['previous_high'].shift(7) / data['previous_high'] - 1\n",
    "    data['high_lag_10'] = data['previous_high'].shift(10) / data['previous_high'] - 1\n",
    "    data['high_lag_14'] = data['previous_high'].shift(14) / data['previous_high'] - 1\n",
    "    data['high_lag_21'] = data['previous_high'].shift(21) / data['previous_high'] - 1\n",
    "    data['high_lag_31'] = data['previous_high'].shift(31) / data['previous_high'] - 1\n",
    "    data['high_lag_50'] = data['previous_high'].shift(50) / data['previous_high'] - 1\n",
    "    data['high_lag_100'] = data['previous_high'].shift(100) / data['previous_high'] - 1\n",
    "    data['low_lag_1'] = data['previous_low'].shift(1) / data['previous_low'] - 1\n",
    "    data['low_lag_7'] = data['previous_low'].shift(7) / data['previous_low'] - 1\n",
    "    data['low_lag_14'] = data['previous_low'].shift(14) / data['previous_low'] - 1\n",
    "    data['low_lag_21'] = data['previous_low'].shift(21) / data['previous_low'] - 1\n",
    "    data['low_lag_31'] = data['previous_low'].shift(31) / data['previous_low'] - 1\n",
    "    data['low_lag_50'] = data['previous_low'].shift(50) / data['previous_low'] - 1\n",
    "    data['low_lag_100'] = data['previous_low'].shift(100) / data['previous_low'] - 1\n",
    "    data['ema_3'] = ta.ema(data['previous_close'], 3)\n",
    "    data['ema_5'] = ta.ema(data['previous_close'], 5)\n",
    "    data['ema_7'] = ta.ema(data['previous_close'], 7)\n",
    "    data['ema_10'] = ta.ema(data['previous_close'], 10)\n",
    "    data['ema_14'] = ta.ema(data['previous_close'], 14)\n",
    "    data['ema_21'] = ta.ema(data['previous_close'], 21)\n",
    "    data['ema_32'] = ta.ema(data['previous_close'], 32)\n",
    "    data['ema_50'] = ta.ema(data['previous_close'], 50)\n",
    "    data['ema_100'] = ta.ema(data['previous_close'], 100)\n",
    "    data['cross_ema_5_3'] = np.where(data['ema_5'] > data['ema_3'], 1, 0)\n",
    "    data['cross_ema_7_3'] = np.where(data['ema_7'] > data['ema_3'], 1, 0)\n",
    "    data['cross_ema_10_3'] = np.where(data['ema_10'] > data['ema_3'], 1, 0)\n",
    "    data['cross_ema_14_3'] = np.where(data['ema_14'] > data['ema_3'], 1, 0)\n",
    "    data['cross_ema_21_3'] = np.where(data['ema_21'] > data['ema_3'], 1, 0)\n",
    "    data['cross_ema_32_3'] = np.where(data['ema_32'] > data['ema_3'], 1, 0)\n",
    "    data['cross_ema_50_3'] = np.where(data['ema_50'] > data['ema_3'], 1, 0)\n",
    "    data['cross_ema_100_3'] = np.where(data['ema_100'] > data['ema_3'], 1, 0)\n",
    "    data['cross_ema_7_5'] = np.where(data['ema_7'] > data['ema_5'], 1, 0)\n",
    "    data['cross_ema_10_5'] = np.where(data['ema_10'] > data['ema_5'], 1, 0)\n",
    "    data['cross_ema_14_5'] = np.where(data['ema_14'] > data['ema_5'], 1, 0)\n",
    "    data['cross_ema_21_5'] = np.where(data['ema_21'] > data['ema_5'], 1, 0)\n",
    "    data['cross_ema_32_5'] = np.where(data['ema_32'] > data['ema_5'], 1, 0)\n",
    "    data['cross_ema_50_5'] = np.where(data['ema_50'] > data['ema_5'], 1, 0)\n",
    "    data['cross_ema_100_5'] = np.where(data['ema_100'] > data['ema_5'], 1, 0)\n",
    "    data['cross_ema_10_7'] = np.where(data['ema_10'] > data['ema_7'], 1, 0)\n",
    "    data['cross_ema_14_7'] = np.where(data['ema_14'] > data['ema_7'], 1, 0)\n",
    "    data['cross_ema_21_7'] = np.where(data['ema_21'] > data['ema_7'], 1, 0)\n",
    "    data['cross_ema_32_7'] = np.where(data['ema_32'] > data['ema_7'], 1, 0)\n",
    "    data['cross_ema_50_7'] = np.where(data['ema_50'] > data['ema_7'], 1, 0)\n",
    "    data['cross_ema_100_7'] = np.where(data['ema_100'] > data['ema_7'], 1, 0)\n",
    "    data['cross_ema_14_10'] = np.where(data['ema_14'] > data['ema_10'], 1, 0)\n",
    "    data['cross_ema_21_10'] = np.where(data['ema_21'] > data['ema_10'], 1, 0)\n",
    "    data['cross_ema_32_10'] = np.where(data['ema_32'] > data['ema_10'], 1, 0)\n",
    "    data['cross_ema_50_10'] = np.where(data['ema_50'] > data['ema_10'], 1, 0)\n",
    "    data['cross_ema_100_10'] = np.where(data['ema_100'] > data['ema_10'], 1, 0)\n",
    "    data['cross_ema_21_14'] = np.where(data['ema_21'] > data['ema_14'], 1, 0)\n",
    "    data['cross_ema_32_14'] = np.where(data['ema_32'] > data['ema_14'], 1, 0)\n",
    "    data['cross_ema_50_14'] = np.where(data['ema_50'] > data['ema_14'], 1, 0)\n",
    "    data['cross_ema_100_14'] = np.where(data['ema_100'] > data['ema_14'], 1, 0)\n",
    "    data['cross_ema_32_21'] = np.where(data['ema_32'] > data['ema_21'], 1, 0)\n",
    "    data['cross_ema_50_21'] = np.where(data['ema_50'] > data['ema_21'], 1, 0)\n",
    "    data['cross_ema_100_21'] = np.where(data['ema_100'] > data['ema_21'], 1, 0)\n",
    "    data['cross_ema_50_32'] = np.where(data['ema_50'] > data['ema_32'], 1, 0)\n",
    "    data['cross_ema_100_32'] = np.where(data['ema_100'] > data['ema_32'], 1, 0)\n",
    "    data['cross_ema_100_50'] = np.where(data['ema_100'] > data['ema_50'], 1, 0)\n",
    "\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(train, test, FEATURES, TARGET, min_child_weight, reg_alpha):\n",
    "    # building XGBoost model - you can change number of n_estimators (model is set up for 10% of stopping rounds depending on number of n_estimators)\n",
    "\n",
    "    X_train = train[FEATURES]\n",
    "    y_train = train[TARGET]\n",
    "\n",
    "    X_test = test[FEATURES]\n",
    "    y_test = test[TARGET]\n",
    "\n",
    "    n_estimators=2500\n",
    "\n",
    "    xgb_model = xgboost.XGBRegressor(learning_rate=0.01, device='cuda', n_estimators=n_estimators, seed=42, early_stopping_rounds=n_estimators*0.1,\n",
    "                                     min_child_weight=min_child_weight, reg_alpha=reg_alpha, max_depth=0)\n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction(test_set, xgb_model, FEATURES):\n",
    "\n",
    "# function for making prediction, all measurments are claculated with absolute values, prediction also gives you information either you shoud place long or short position,\n",
    "# it calculates mean error in terms of price, but also accuracy depending on which position was predicted by model vs actual data\n",
    "\n",
    "    future = pd.date_range(test_set.at[0, 'Date'], test_set.at[max(test_set.index), 'Date'], freq='D')\n",
    "    future_df = pd.DataFrame(future)\n",
    "\n",
    "    test_set['pred'] = xgb_model.predict(test_set[FEATURES])\n",
    "\n",
    "    prediction = test_set[['Close', 'pred', 'Open']]\n",
    "\n",
    "    prediction['abs_accuracy'] = np.abs((test_set.Close / test_set.pred - 1) * 100)\n",
    "\n",
    "    prediction['position'] = np.where(prediction['Close'] > prediction['Open'], 'long', 'short')\n",
    "    prediction['predicted_position'] = np.where(prediction['pred'] > prediction['Open'], 'long', 'short')\n",
    "    prediction['accuracy'] = np.where(prediction['position'] == prediction['predicted_position'], 1, 0)\n",
    "\n",
    "    mean_error = prediction['abs_accuracy'].mean()\n",
    "    mean_accuracy_position = prediction.accuracy.sum() / prediction.accuracy.count()\n",
    "\n",
    "\n",
    "    return mean_error, mean_accuracy_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in ticker_list:\n",
    "    try:\n",
    "        df = download_data(ticker, '2015-01-01')\n",
    "        data_w_features = create_features(df)\n",
    "\n",
    "        features = list(data_w_features.columns)\n",
    "\n",
    "        FEATURES = features\n",
    "        TARGET = ['Close']\n",
    "            \n",
    "    # data split for 7 training folds with 10 days of data each. You can use TimeSeriesSplit function from sklearn to achieve same thing\n",
    "        \n",
    "        train_1 = data_w_features[:-70]\n",
    "        test_1 = data_w_features[-70:-60]\n",
    "\n",
    "        train_2 = data_w_features[:-60]\n",
    "        test_2 = data_w_features[-60:-50]\n",
    "\n",
    "        train_3 = data_w_features[:-50]\n",
    "        test_3 = data_w_features[-50:-40]\n",
    "\n",
    "        train_4 = data_w_features[:-40]\n",
    "        test_4 = data_w_features[-40:-30]\n",
    "\n",
    "        train_5 = data_w_features[:-30]\n",
    "        test_5 = data_w_features[-30:-20]\n",
    "\n",
    "        train_6 = data_w_features[:-20]\n",
    "        test_6 = data_w_features[-20:-10]\n",
    "\n",
    "        train_7 = data_w_features[:-10]\n",
    "        test_7 = data_w_features[-10:]\n",
    "\n",
    "        train_test_dict = {\n",
    "            1: [train_1, test_1],\n",
    "            2: [train_2, test_2],\n",
    "            3: [train_3, test_3],\n",
    "            4: [train_4, test_4],\n",
    "            5: [train_5, test_5],\n",
    "            6: [train_6, test_6],\n",
    "            7: [train_7, test_7]\n",
    "        }\n",
    "\n",
    "\n",
    "        features.remove('Close')\n",
    "        features.remove('Open')\n",
    "        features.remove('High')\n",
    "        features.remove('Low')\n",
    "        features.remove('Date')\n",
    "\n",
    "\n",
    "        for i in range(1, 8):\n",
    "            train = train_test_dict[i][0]\n",
    "            test = train_test_dict[i][1]\n",
    "\n",
    "            test.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "        for min_child_weight in params['min_child_weight']:\n",
    "            for reg_alpha in params['reg_alpha']:\n",
    "                for i in range(1, 8):\n",
    "\n",
    "                    average_scores = pd.DataFrame(columns=['ticker', 'train_fold', 'mean_error', 'mean_accuracy_position', 'min_child_weight', 'reg_alpha'])\n",
    "\n",
    "                    train = train_test_dict[i][0]\n",
    "                    test = train_test_dict[i][1]\n",
    "\n",
    "                    xgb_model = create_model(train, test, FEATURES, TARGET, min_child_weight, reg_alpha)\n",
    "\n",
    "                    mean_error, mean_accuracy_position = create_prediction(test, xgb_model, FEATURES)\n",
    "\n",
    "                    average_scores.loc[0] = ticker, i, mean_error, mean_accuracy_position, min_child_weight, reg_alpha\n",
    "                    \n",
    "\n",
    "                    average_scores.to_sql('hyper_xgboost_all_tickers', engine, if_exists='append')\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgboost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
